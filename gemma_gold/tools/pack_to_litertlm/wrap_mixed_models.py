import os

from ai_edge_litert.internal import (
    litertlm_builder,
    llm_metadata_pb2,
    llm_model_type_pb2,
)

# --- Path Configurations ---
BASE_DIR = "/home/pilmo/workspace/ai-edge-torch"
REF_DIR = os.path.join(BASE_DIR, "pilmo_test_lab_literlm", "reference", "succeed_model")
# Inputs: Models exported from previous steps
SOURCE_MODELS_DIR = os.path.join(BASE_DIR, "gemma3_quant_test", "output")
# Outputs: Final .litertlm package location
FINAL_PACKAGE_DIR = os.path.join(
    BASE_DIR, "pilmo_test_lab_literlm", "bin", "mixed_test"
)

# Component Paths
TOKENIZER_PATH = os.path.join(
    BASE_DIR,
    "LiteRT-LM",
    "runtime",
    "components",
    "testdata",
    "gemma3_sentencepiece.model",
)
EMBEDDER_PATH = os.path.join(REF_DIR, "real_embedding.tflite")
AUX_PATH = os.path.join(REF_DIR, "real_aux.tflite")
MAIN_MODEL_PATH = os.path.join(
    SOURCE_MODELS_DIR,
    "pilmo_optimized_main_w4a16_aot_FINAL.tflite",
)

# Output LiteRT-LM Paths (Generated by this script)
LITERT_LM_OUTPUT_PATH = os.path.join(FINAL_PACKAGE_DIR, "PILMO_LOVE_GEMMA.litertlm")
METADATA_PB_PATH = os.path.join(FINAL_PACKAGE_DIR, "mixed_metadata.pb")


def build_mixed_litertlm():
    if not os.path.exists(FINAL_PACKAGE_DIR):
        os.makedirs(FINAL_PACKAGE_DIR)

    # Verify paths
    for p in [TOKENIZER_PATH, EMBEDDER_PATH, AUX_PATH, MAIN_MODEL_PATH]:
        if not os.path.exists(p):
            print(f"ERROR: Path {p} does not exist.")
            # If our main model is missing, we might need to run the export script.
            if p == MAIN_MODEL_PATH:
                print("Hint: Run gemma3_quant_test/final_optimized_export_v3.py first.")
            return

    # 1. Create LLM Metadata (Same as reference spec)
    print("Generating LLM Metadata...")
    llm_metadata = llm_metadata_pb2.LlmMetadata()
    llm_metadata.max_num_tokens = 1280
    llm_metadata.start_token.token_ids.ids.append(2)
    for tid in [1, 106, 107]:
        stop_tok = llm_metadata.stop_tokens.add()
        stop_tok.token_ids.ids.append(tid)
    llm_metadata.llm_model_type.gemma3.CopyFrom(llm_model_type_pb2.Gemma3())

    with open(METADATA_PB_PATH, "wb") as f:
        f.write(llm_metadata.SerializeToString())

    # 2. Initialize the Builder
    builder = litertlm_builder.LitertLmFileBuilder()

    # 3. Add System Metadata
    builder.add_system_metadata(
        litertlm_builder.Metadata(
            key="Authors",
            value="Zetic.ai (Mixed Verification)",
            dtype=litertlm_builder.DType.STRING,
        )
    )

    # 4. Add LLM Metadata and Tokenizer
    print(f"Adding LLM Metadata: {METADATA_PB_PATH}")
    builder.add_llm_metadata(METADATA_PB_PATH)

    print(f"Adding Tokenizer: {TOKENIZER_PATH}")
    builder.add_sentencepiece_tokenizer(TOKENIZER_PATH)

    # 5. Add Models
    print("Adding Models...")
    print(f"  [Embedder (REF)]: {EMBEDDER_PATH}")
    builder.add_tflite_model(EMBEDDER_PATH, litertlm_builder.TfLiteModelType.EMBEDDER)

    print(f"  [Aux (REF)]: {AUX_PATH}")
    builder.add_tflite_model(AUX_PATH, litertlm_builder.TfLiteModelType.AUX)

    print(f"  [Main (OURS)]: {MAIN_MODEL_PATH}")
    builder.add_tflite_model(
        MAIN_MODEL_PATH, litertlm_builder.TfLiteModelType.PREFILL_DECODE
    )

    # 6. Final Build
    print(f"Building mixed .litertlm at {LITERT_LM_OUTPUT_PATH}...")
    try:
        with open(LITERT_LM_OUTPUT_PATH, "wb") as f:
            builder.build(f)
        print("\n" + "=" * 50)
        print("SUCCESS! Mixed LiteRT-LM Package is ready.")
        print(f"Output: {LITERT_LM_OUTPUT_PATH}")
        print("=" * 50)
    except Exception as e:
        print(f"BUILD ERROR: {e}")


if __name__ == "__main__":
    build_mixed_litertlm()
